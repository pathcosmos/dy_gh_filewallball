#!/usr/bin/env python3
"""
FileWallBall API - ì„±ëŠ¥ ë° ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” APIì˜ ì„±ëŠ¥, ë™ì‹œ ì ‘ì† ì²˜ë¦¬ ëŠ¥ë ¥, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë“±ì„
í…ŒìŠ¤íŠ¸í•˜ì—¬ ì‹œìŠ¤í…œì˜ ì•ˆì •ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import os
import time
import requests
import json
import threading
import concurrent.futures
import statistics
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import psutil
import subprocess
import tempfile

class PerformanceTester:
    def __init__(self, api_base_url: str = "http://127.0.0.1:8000"):
        self.api_base_url = api_base_url
        self.test_results = {}
        self.start_time = time.time()
        self.process = psutil.Process()
        
    def log(self, message: str, level: str = "INFO"):
        """ë¡œê·¸ ë©”ì‹œì§€ ì¶œë ¥"""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] [{level}] {message}")
    
    def get_system_stats(self) -> Dict:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        return {
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "memory_available_gb": memory.available / (1024**3),
            "disk_percent": disk.percent,
            "disk_free_gb": disk.free / (1024**3)
        }
    
    def test_health_endpoint_performance(self, num_requests: int = 100) -> Dict:
        """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        self.log(f"\n--- í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ({num_requests}íšŒ) ---")
        
        response_times = []
        success_count = 0
        error_count = 0
        
        start_time = time.time()
        
        for i in range(num_requests):
            try:
                request_start = time.time()
                response = requests.get(f"{self.api_base_url}/health", timeout=5)
                request_time = time.time() - request_start
                
                if response.status_code == 200:
                    success_count += 1
                    response_times.append(request_time * 1000)  # Convert to milliseconds
                else:
                    error_count += 1
                    
            except Exception as e:
                error_count += 1
                self.log(f"ìš”ì²­ {i+1} ì‹¤íŒ¨: {e}", "ERROR")
        
        total_time = time.time() - start_time
        
        if response_times:
            avg_response_time = statistics.mean(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18]  # 95th percentile
            min_response_time = min(response_times)
            max_response_time = max(response_times)
        else:
            avg_response_time = p95_response_time = min_response_time = max_response_time = 0
        
        tps = success_count / total_time if total_time > 0 else 0
        
        result = {
            "test_name": "í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸",
            "total_requests": num_requests,
            "success_count": success_count,
            "error_count": error_count,
            "success_rate": (success_count / num_requests) * 100,
            "total_time_seconds": total_time,
            "tps": tps,
            "response_times_ms": {
                "average": round(avg_response_time, 2),
                "p95": round(p95_response_time, 2),
                "min": round(min_response_time, 2),
                "max": round(max_response_time, 2)
            }
        }
        
        self.log(f"âœ… ì„±ê³µ: {success_count}/{num_requests} ({result['success_rate']:.1f}%)")
        self.log(f"ğŸ“Š TPS: {tps:.2f}")
        self.log(f"â±ï¸ í‰ê·  ì‘ë‹µì‹œê°„: {avg_response_time:.2f}ms")
        self.log(f"ğŸ“ˆ 95í¼ì„¼íƒ€ì¼: {p95_response_time:.2f}ms")
        
        return result
    
    def test_concurrent_uploads(self, num_concurrent: int = 10, files_per_batch: int = 5) -> Dict:
        """ë™ì‹œ ì—…ë¡œë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        self.log(f"\n--- ë™ì‹œ ì—…ë¡œë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ë™ì‹œ {num_concurrent}ê°œ, ë°°ì¹˜ë‹¹ {files_per_batch}ê°œ) ---")
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ ìƒì„±
        test_files = []
        for i in range(num_concurrent * files_per_batch):
            file_path = Path(f"perf_test_{i}.txt")
            content = f"Performance test file {i}\n" * 100  # ~2KB íŒŒì¼
            file_path.write_text(content)
            test_files.append(file_path)
        
        upload_results = []
        start_time = time.time()
        
        def upload_batch(file_batch: List[Path]) -> List[Dict]:
            batch_results = []
            for file_path in file_batch:
                try:
                    upload_start = time.time()
                    with open(file_path, 'rb') as f:
                        files = {'file': (file_path.name, f, 'text/plain')}
                        response = requests.post(f"{self.api_base_url}/upload", files=files, timeout=30)
                    
                    upload_time = time.time() - upload_start
                    
                    if response.status_code == 200:
                        data = response.json()
                        batch_results.append({
                            "filename": file_path.name,
                            "success": True,
                            "file_id": data.get('file_id'),
                            "upload_time_seconds": upload_time,
                            "file_size_bytes": os.path.getsize(file_path)
                        })
                    else:
                        batch_results.append({
                            "filename": file_path.name,
                            "success": False,
                            "status_code": response.status_code,
                            "error": response.text[:100]
                        })
                        
                except Exception as e:
                    batch_results.append({
                        "filename": file_path.name,
                        "success": False,
                        "error": str(e)
                    })
            
            return batch_results
        
        try:
            # íŒŒì¼ë“¤ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
            batches = [test_files[i:i + files_per_batch] for i in range(0, len(test_files), files_per_batch)]
            
            # ë™ì‹œ ì—…ë¡œë“œ ì‹¤í–‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=num_concurrent) as executor:
                future_to_batch = {executor.submit(upload_batch, batch): batch for batch in batches}
                
                for future in concurrent.futures.as_completed(future_to_batch):
                    batch_results = future.result()
                    upload_results.extend(batch_results)
            
            total_time = time.time() - start_time
            
            # ê²°ê³¼ ë¶„ì„
            successful_uploads = [r for r in upload_results if r.get('success', False)]
            failed_uploads = [r for r in upload_results if not r.get('success', False)]
            
            if successful_uploads:
                upload_times = [r['upload_time_seconds'] for r in successful_uploads]
                avg_upload_time = statistics.mean(upload_times)
                p95_upload_time = statistics.quantiles(upload_times, n=20)[18] if len(upload_times) >= 20 else max(upload_times)
                total_file_size = sum(r['file_size_bytes'] for r in successful_uploads)
                throughput_mbps = (total_file_size / total_time) / (1024 * 1024) if total_time > 0 else 0
            else:
                avg_upload_time = p95_upload_time = throughput_mbps = 0
            
            result = {
                "test_name": "ë™ì‹œ ì—…ë¡œë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸",
                "concurrent_workers": num_concurrent,
                "files_per_batch": files_per_batch,
                "total_files": len(test_files),
                "successful_uploads": len(successful_uploads),
                "failed_uploads": len(failed_uploads),
                "success_rate": (len(successful_uploads) / len(test_files)) * 100,
                "total_time_seconds": total_time,
                "avg_upload_time_seconds": round(avg_upload_time, 3),
                "p95_upload_time_seconds": round(p95_upload_time, 3),
                "throughput_mbps": round(throughput_mbps, 2),
                "upload_results": upload_results[:10]  # ì²˜ìŒ 10ê°œë§Œ ì €ì¥ (ì „ì²´ëŠ” ë„ˆë¬´ í¼)
            }
            
            self.log(f"âœ… ì„±ê³µ: {len(successful_uploads)}/{len(test_files)} ({result['success_rate']:.1f}%)")
            self.log(f"â±ï¸ í‰ê·  ì—…ë¡œë“œ ì‹œê°„: {avg_upload_time:.3f}s")
            self.log(f"ğŸ“ˆ 95í¼ì„¼íƒ€ì¼: {p95_upload_time:.3f}s")
            self.log(f"ğŸš€ ì²˜ë¦¬ëŸ‰: {throughput_mbps:.2f} MB/s")
            
        except Exception as e:
            self.log(f"âŒ ë™ì‹œ ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", "ERROR")
            result = {
                "test_name": "ë™ì‹œ ì—…ë¡œë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸",
                "error": str(e),
                "success": False
            }
        
        finally:
            # í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ ì •ë¦¬
            for file_path in test_files:
                if file_path.exists():
                    file_path.unlink()
        
        return result
    
    def test_memory_usage_during_load(self, duration_seconds: int = 60) -> Dict:
        """ë¶€í•˜ ìƒíƒœì—ì„œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        self.log(f"\n--- ë¶€í•˜ ìƒíƒœì—ì„œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ({duration_seconds}ì´ˆ) ---")
        
        memory_samples = []
        start_time = time.time()
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§€ì†ì ì¸ ìš”ì²­ ìƒì„±
        def background_requests():
            while time.time() - start_time < duration_seconds:
                try:
                    requests.get(f"{self.api_base_url}/health", timeout=2)
                    time.sleep(0.1)  # 100ms ê°„ê²©
                except:
                    pass
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitor_thread = threading.Thread(target=background_requests)
        monitor_thread.start()
        
        try:
            while time.time() - start_time < duration_seconds:
                # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                process_memory = self.process.memory_info()
                system_memory = psutil.virtual_memory()
                
                memory_samples.append({
                    "timestamp": time.time() - start_time,
                    "process_rss_mb": process_memory.rss / (1024 * 1024),
                    "process_vms_mb": process_memory.vms / (1024 * 1024),
                    "system_memory_percent": system_memory.percent,
                    "system_memory_available_gb": system_memory.available / (1024**3)
                })
                
                time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ìƒ˜í”Œë§
                
        except KeyboardInterrupt:
            self.log("ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨ë¨")
        
        monitor_thread.join()
        
        if memory_samples:
            rss_values = [s['process_rss_mb'] for s in memory_samples]
            vms_values = [s['process_vms_mb'] for s in memory_samples]
            system_memory_percent = [s['system_memory_percent'] for s in memory_samples]
            
            result = {
                "test_name": "ë¶€í•˜ ìƒíƒœì—ì„œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§",
                "duration_seconds": duration_seconds,
                "sample_count": len(memory_samples),
                "process_memory_mb": {
                    "rss": {
                        "min": round(min(rss_values), 2),
                        "max": round(max(rss_values), 2),
                        "average": round(statistics.mean(rss_values), 2),
                        "p95": round(statistics.quantiles(rss_values, n=20)[18] if len(rss_values) >= 20 else max(rss_values), 2)
                    },
                    "vms": {
                        "min": round(min(vms_values), 2),
                        "max": round(max(vms_values), 2),
                        "average": round(statistics.mean(vms_values), 2),
                        "p95": round(statistics.quantiles(vms_values, n=20)[18] if len(vms_values) >= 20 else max(vms_values), 2)
                    }
                },
                "system_memory_percent": {
                    "min": round(min(system_memory_percent), 2),
                    "max": round(max(system_memory_percent), 2),
                    "average": round(statistics.mean(system_memory_percent), 2)
                },
                "memory_samples": memory_samples[::5]  # 5ì´ˆë§ˆë‹¤ ìƒ˜í”Œë§í•˜ì—¬ ì €ì¥
            }
            
            self.log(f"ğŸ“Š í”„ë¡œì„¸ìŠ¤ RSS: {result['process_memory_mb']['rss']['min']:.1f}MB ~ {result['process_memory_mb']['rss']['max']:.1f}MB")
            self.log(f"ğŸ“Š í”„ë¡œì„¸ìŠ¤ VMS: {result['process_memory_mb']['vms']['min']:.1f}MB ~ {result['process_memory_mb']['vms']['max']:.1f}MB")
            self.log(f"ğŸ“Š ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {result['system_memory_percent']['min']:.1f}% ~ {result['system_memory_percent']['max']:.1f}%")
            
        else:
            result = {
                "test_name": "ë¶€í•˜ ìƒíƒœì—ì„œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§",
                "error": "ë©”ëª¨ë¦¬ ìƒ˜í”Œ ìˆ˜ì§‘ ì‹¤íŒ¨",
                "success": False
            }
        
        return result
    
    def test_apache_bench(self, num_requests: int = 1000, concurrency: int = 10) -> Dict:
        """Apache Benchë¥¼ ì‚¬ìš©í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        self.log(f"\n--- Apache Bench ë¶€í•˜ í…ŒìŠ¤íŠ¸ ({num_requests} ìš”ì²­, ë™ì‹œ {concurrency}) ---")
        
        # ab ëª…ë ¹ì–´ í™•ì¸
        try:
            subprocess.run(["ab", "-V"], capture_output=True, check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            self.log("âŒ Apache Bench (ab)ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.", "WARNING")
            return {
                "test_name": "Apache Bench ë¶€í•˜ í…ŒìŠ¤íŠ¸",
                "error": "Apache Bench (ab) not installed",
                "success": False
            }
        
        try:
            # í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¶€í•˜ í…ŒìŠ¤íŠ¸
            cmd = [
                "ab", "-n", str(num_requests), "-c", str(concurrency),
                "-g", "ab_results.tsv",  # ê²°ê³¼ë¥¼ TSV íŒŒì¼ë¡œ ì €ì¥
                f"{self.api_base_url}/health"
            ]
            
            self.log(f"ì‹¤í–‰ ëª…ë ¹ì–´: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                # ê²°ê³¼ íŒŒì‹±
                output_lines = result.stdout.split('\n')
                ab_results = {}
                
                for line in output_lines:
                    if 'Requests per second:' in line:
                        ab_results['rps'] = float(line.split(':')[1].strip().split()[0])
                    elif 'Time per request:' in line:
                        ab_results['time_per_request_ms'] = float(line.split(':')[1].strip().split()[0])
                    elif 'Transfer rate:' in line:
                        ab_results['transfer_rate_kbps'] = float(line.split(':')[1].strip().split()[0])
                    elif 'Percentage of the requests served within a certain time' in line:
                        # 95í¼ì„¼íƒ€ì¼ ì‹œê°„ ì°¾ê¸°
                        for i, l in enumerate(output_lines):
                            if '95%' in l:
                                ab_results['p95_time_ms'] = float(l.split()[1])
                                break
                
                # TSV íŒŒì¼ì—ì„œ ìƒì„¸ ê²°ê³¼ ì½ê¸°
                if os.path.exists("ab_results.tsv"):
                    with open("ab_results.tsv", 'r') as f:
                        lines = f.readlines()
                        if len(lines) > 1:  # í—¤ë” + ë°ì´í„°
                            response_times = []
                            for line in lines[1:]:  # í—¤ë” ì œì™¸
                                parts = line.strip().split('\t')
                                if len(parts) >= 2:
                                    try:
                                        response_times.append(float(parts[1]))
                                    except ValueError:
                                        continue
                            
                            if response_times:
                                ab_results['response_times_ms'] = {
                                    'min': min(response_times),
                                    'max': max(response_times),
                                    'mean': statistics.mean(response_times),
                                    'median': statistics.median(response_times)
                                }
                
                result_data = {
                    "test_name": "Apache Bench ë¶€í•˜ í…ŒìŠ¤íŠ¸",
                    "num_requests": num_requests,
                    "concurrency": concurrency,
                    "success": True,
                    "ab_results": ab_results,
                    "raw_output": result.stdout[:1000]  # ì²˜ìŒ 1000ìë§Œ ì €ì¥
                }
                
                self.log(f"âœ… RPS: {ab_results.get('rps', 'N/A')}")
                self.log(f"â±ï¸ í‰ê·  ì‘ë‹µì‹œê°„: {ab_results.get('time_per_request_ms', 'N/A')}ms")
                if 'p95_time_ms' in ab_results:
                    self.log(f"ğŸ“ˆ 95í¼ì„¼íƒ€ì¼: {ab_results['p95_time_ms']:.2f}ms")
                
                return result_data
                
            else:
                self.log(f"âŒ Apache Bench ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}", "ERROR")
                return {
                    "test_name": "Apache Bench ë¶€í•˜ í…ŒìŠ¤íŠ¸",
                    "error": result.stderr,
                    "success": False
                }
                
        except subprocess.TimeoutExpired:
            self.log("âŒ Apache Bench í…ŒìŠ¤íŠ¸ ì‹œê°„ ì´ˆê³¼", "ERROR")
            return {
                "test_name": "Apache Bench ë¶€í•˜ í…ŒìŠ¤íŠ¸",
                "error": "Timeout expired",
                "success": False
            }
        except Exception as e:
            self.log(f"âŒ Apache Bench í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", "ERROR")
            return {
                "test_name": "Apache Bench ë¶€í•˜ í…ŒìŠ¤íŠ¸",
                "error": str(e),
                "success": False
            }
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists("ab_results.tsv"):
                os.remove("ab_results.tsv")
    
    def run_all_tests(self):
        """ëª¨ë“  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        self.log("ğŸš€ FileWallBall API ì„±ëŠ¥ ë° ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ì´ˆê¸° ì‹œìŠ¤í…œ ìƒíƒœ
        initial_stats = self.get_system_stats()
        self.log(f"ì´ˆê¸° ì‹œìŠ¤í…œ ìƒíƒœ: CPU {initial_stats['cpu_percent']:.1f}%, ë©”ëª¨ë¦¬ {initial_stats['memory_percent']:.1f}%")
        
        # API í—¬ìŠ¤ì²´í¬
        try:
            response = requests.get(f"{self.api_base_url}/health", timeout=5)
            if response.status_code != 200:
                self.log("âŒ APIê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.", "ERROR")
                return
        except Exception as e:
            self.log(f"âŒ API ì—°ê²° ì‹¤íŒ¨: {e}. í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.", "ERROR")
            return
        
        # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        tests = [
            self.test_health_endpoint_performance,
            self.test_concurrent_uploads,
            self.test_memory_usage_during_load,
            self.test_apache_bench
        ]
        
        for test_func in tests:
            try:
                self.log(f"\n{'='*60}")
                test_result = test_func()
                self.test_results[test_result["test_name"]] = test_result
                
                # í…ŒìŠ¤íŠ¸ ê°„ ì ì‹œ ëŒ€ê¸°
                time.sleep(2)
                
            except Exception as e:
                self.log(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {test_func.__name__} - {e}", "ERROR")
                self.test_results[test_func.__name__] = {
                    "test_name": test_func.__name__,
                    "status": "ERROR",
                    "error": str(e)
                }
        
        # ìµœì¢… ì‹œìŠ¤í…œ ìƒíƒœ
        final_stats = self.get_system_stats()
        
        # ê²°ê³¼ ìš”ì•½
        self.log("\n" + "="*60)
        self.log("ğŸ ì„±ëŠ¥ ë° ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ ìš”ì•½")
        self.log("="*60)
        
        total_tests = len(self.test_results)
        successful_tests = sum(1 for r in self.test_results.values() if r.get("success", True))
        failed_tests = total_tests - successful_tests
        
        self.log(f"ì´ í…ŒìŠ¤íŠ¸: {total_tests}")
        self.log(f"ì„±ê³µ: {successful_tests}")
        self.log(f"ì‹¤íŒ¨: {failed_tests}")
        
        # ìƒì„¸ ê²°ê³¼
        for test_name, result in self.test_results.items():
            status_icon = "âœ…" if result.get("success", True) else "âŒ"
            self.log(f"{status_icon} {test_name}: {'ì„±ê³µ' if result.get('success', True) else 'ì‹¤íŒ¨'}")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ ë³€í™”
        self.log(f"\nğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ ë³€í™”:")
        self.log(f"CPU: {initial_stats['cpu_percent']:.1f}% â†’ {final_stats['cpu_percent']:.1f}%")
        self.log(f"ë©”ëª¨ë¦¬: {initial_stats['memory_percent']:.1f}% â†’ {final_stats['memory_percent']:.1f}%")
        self.log(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {initial_stats['memory_available_gb']:.2f}GB â†’ {final_stats['memory_available_gb']:.2f}GB")
        
        # ê²°ê³¼ ì €ì¥
        with open("performance_test_results.json", "w", encoding="utf-8") as f:
            json.dump(self.test_results, f, indent=4, ensure_ascii=False)
        self.log("\ní…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ performance_test_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        end_time = time.time()
        total_duration = end_time - self.start_time
        self.log(f"ì´ ì†Œìš” ì‹œê°„: {total_duration:.2f}ì´ˆ")

if __name__ == "__main__":
    tester = PerformanceTester()
    tester.run_all_tests()
